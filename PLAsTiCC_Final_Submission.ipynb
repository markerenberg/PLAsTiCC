{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLAsTiCC Final Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVLTfzjk85GOxLhXTy8QNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markerenberg/PLAsTiCC/blob/master/PLAsTiCC_Final_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjvw1b2GVXj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tsfresh.feature_extraction import extract_features\n",
        "import autograd.numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import pywt\n",
        "import warnings\n",
        "import emcee\n",
        "import celerite\n",
        "from celerite import terms\n",
        "from matplotlib.mlab import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43VroByVnLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir('C:\\\\Users\\\\marke\\\\Downloads\\\\PLAsTiCC')\n",
        "train = pd.read_csv('training_set.csv')\n",
        "train_meta = pd.read_csv('training_set_metadata.csv')\n",
        "test_meta = pd.read_csv('test_set_metadata.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIVFacp2VsNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## =======================================================\n",
        "## Feature Engineering\n",
        "## =======================================================\n",
        "\n",
        "## subset metadata:\n",
        "#train_meta = train_meta[train_meta['target'].isin([42,52,62,67,90])]\n",
        "#train = train[train['object_id'].isin(train_meta['object_id'].unique())]\n",
        "\n",
        "gc.enable()\n",
        "\n",
        "\n",
        "#### Gaussian Process Regression ####\n",
        "\n",
        "# Estimate the MAP parameters using L-BFGS-B\n",
        "def nll(p, y, gp):\n",
        "    # Update the kernel parameters:\n",
        "    gp.set_parameter_vector(p)\n",
        "    #  Compute the loglikelihood:\n",
        "    ll = gp.log_likelihood(y,quiet=True)\n",
        "    # The scipy optimizer doesnâ€™t play well with infinities:\n",
        "    return -ll if np.isfinite(ll) else 1e25\n",
        "def grad_nll(p, y, gp):\n",
        "    # Update the kernel parameters:\n",
        "    gp.set_parameter_vector(p)\n",
        "    #  Compute the gradient of the loglikelihood:\n",
        "    gll = gp.grad_log_likelihood(y,quiet=True)[1]\n",
        "    return -gll\n",
        "\n",
        "# set the loglikelihood:\n",
        "def lnlike(p, x, y,gp):\n",
        "    ln_a = p[0]\n",
        "    ln_b = p[1]\n",
        "    p0 = np.array([ln_a,ln_b])\n",
        "    # update kernel parameters:\n",
        "    gp.set_parameter_vector(p0)\n",
        "    # calculate the likelihood:\n",
        "    ll = gp.log_likelihood(y, quiet=True)\n",
        "    # return\n",
        "    return ll if np.isfinite(ll) else 1e25\n",
        "    \n",
        "# set the logposterior:\n",
        "def lnprob(p, x, y,gp):\n",
        "    #return lp + lnlike(p, x, y,gp) if np.isfinite(lp) else -np.inf\n",
        "    return lnlike(p, x, y,gp)\n",
        "   \n",
        "def wavelet(df):\n",
        "    # Gaussian Regression\n",
        "    result = pd.DataFrame()\n",
        "    mjds = df['mjd'].unique()\n",
        "    # Two observations per unique mjd value\n",
        "    t = np.arange(np.min(mjds),np.max(mjds),0.5)\n",
        "    if (len(t)%2) == 0:\n",
        "        t = np.insert(t,len(t),t[len(t)-1] + 0.5)\n",
        "    for obj, agg_df in df.groupby('object_id'):\n",
        "        agg_df = agg_df.sort_values(by=['mjd'])\n",
        "        X = agg_df['mjd']\n",
        "        Y = agg_df['flux']\n",
        "        Yerr = agg_df['flux_err']\n",
        "        # Start by setting hyperparamaters to unit:\n",
        "        log_sigma = 0\n",
        "        log_rho = 0\n",
        "        kernel = celerite.terms.Matern32Term(log_sigma,log_rho)\n",
        "        # According to the paper from Narayan et al, 2018, we will use the Matern 3/2 Kernel.\n",
        "        gp = celerite.GP(kernel, mean=0.0)\n",
        "        gp.compute(X,Yerr)  \n",
        "        # extract our initial guess at parameters\n",
        "        # from the celerite kernel and put it in a\n",
        "        # vector:\n",
        "        p0 = gp.get_parameter_vector()         \n",
        "        # run optimization:\n",
        "        results = minimize(nll, p0, method='L-BFGS-B', jac=grad_nll, args=(Y, gp))  \n",
        "        # set your initial guess parameters\n",
        "        # as the output from the scipy optimiser\n",
        "        # remember celerite keeps these in ln() form!\n",
        "        gp.set_parameter_vector(np.abs(results.x))\n",
        "        # Predict posterior mean and variance\n",
        "        mu, var = gp.predict(Y, t,return_var=True)\n",
        "        if(sum(np.isnan(mu)) != 0): \n",
        "            print('NANs exist in mu vector')\n",
        "            return [obj,results.x,mu]\n",
        "        # Wavelet Transform\n",
        "        # calculate wavelet transform using even numbered array\n",
        "        (cA2, cD2), (cA1, cD1) = pywt.swt(mu[1:,],'sym2',level=2)\n",
        "        obj_df = pd.DataFrame(list(cA2)+list(cA1)+list(cD2)+list(cD1)).transpose()\n",
        "        obj_df['object_id'] = obj\n",
        "        result = pd.concat([result,obj_df])\n",
        "    result.reset_index(inplace=True)\n",
        "    result.drop(\"index\",axis=1,inplace=True)\n",
        "    return result\n",
        "        \n",
        "        \n",
        "# Extract features from train set:\n",
        "def featurize(df,meta):\n",
        "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
        "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
        "    # train[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]  \n",
        "    aggs = {\n",
        "        'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
        "        'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
        "        'detected': ['mean'],\n",
        "        'flux_ratio_sq':['sum','skew'],\n",
        "        'flux_by_flux_ratio_sq':['sum','skew']}\n",
        "    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity\n",
        "    fcp = {'flux': {'longest_strike_above_mean': None,'longest_strike_below_mean': None,\n",
        "                'mean_change': None,'mean_abs_change': None,'length': None,},\n",
        "        'flux_by_flux_ratio_sq': {'longest_strike_above_mean': None,\n",
        "                                  'longest_strike_below_mean': None,},\n",
        "        'flux_passband': {'fft_coefficient': [{'coeff': 0, 'attr': 'abs'}, {'coeff': 1, 'attr': 'abs'}],\n",
        "                'kurtosis' : None, 'skewness' : None,},\n",
        "        'mjd': {'maximum': None, 'minimum': None,'mean_change': None,'mean_abs_change': None,},}\n",
        "    agg_df = df.groupby(['object_id']).agg(aggs)\n",
        "    new_columns = [\n",
        "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
        "    ]\n",
        "    agg_df.columns = new_columns\n",
        "    agg_df = agg_df.reset_index()\n",
        "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
        "    agg_df['flux_dif2'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
        "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df['flux_ratio_sq_sum']\n",
        "    agg_df['flux_dif3'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
        "    # Added DWT features (ignore warnings from pywt.wavedec):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        wavelet_df = wavelet(df)\n",
        "    agg_df = agg_df.merge(right=wavelet_df,how = 'left',on='object_id')\n",
        "    print('MERGED WAVELET DF')\n",
        "    # Run PCA\n",
        "    X = agg_df.iloc[:, 22:]\n",
        "    X_std = StandardScaler().fit_transform(X)\n",
        "    pca = PCA(n_components=200)\n",
        "    pca_results = pd.DataFrame(pca.fit_transform(X_std))\n",
        "    pca_results.columns = ['PCA' + str(x+1) for x in range(200)]\n",
        "    agg_df = agg_df.iloc[:,:22]\n",
        "    agg_df = pd.concat([agg_df,pca_results],axis=1)\n",
        "    # Add more tsfresh features with passband, flux, flux_ratio_sq:\n",
        "    agg_df_ts_flux_passband = extract_features(df, column_id='object_id', column_sort='mjd', column_kind='passband', column_value = 'flux', default_fc_parameters = fcp['flux_passband'], n_jobs=4)\n",
        "    agg_df_ts_flux = extract_features(df, column_id='object_id', column_value='flux', default_fc_parameters=fcp['flux'], n_jobs=4)\n",
        "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, column_id='object_id', column_value='flux_by_flux_ratio_sq', default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=4)\n",
        "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
        "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
        "    df_det = df[df['detected']==1].copy() \n",
        "    agg_df_mjd = extract_features(df_det, column_id='object_id', column_value = 'mjd', default_fc_parameters = fcp['mjd'], n_jobs=4)\n",
        "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd['mjd__minimum']\n",
        "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum'] \n",
        "    agg_df_ts_flux_passband = agg_df_ts_flux_passband.reset_index()\n",
        "    agg_df_ts_flux_passband.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_ts_flux = agg_df_ts_flux.reset_index()\n",
        "    agg_df_ts_flux.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_ts_flux_by_flux_ratio_sq = agg_df_ts_flux_by_flux_ratio_sq.reset_index()\n",
        "    agg_df_ts_flux_by_flux_ratio_sq.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_mjd = agg_df_mjd.reset_index()\n",
        "    agg_df_mjd.rename(columns={'id':'object_id'},inplace=True) \n",
        "    agg_df_ts = pd.concat([agg_df, \n",
        "                           agg_df_ts_flux_passband.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_ts_flux.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_ts_flux_by_flux_ratio_sq.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_mjd.drop(labels=['object_id'], axis=1)], axis=1).reset_index()\n",
        "    if 'index' in agg_df_ts:\n",
        "       del agg_df_ts['index'] \n",
        "    result = agg_df_ts.merge(right=meta, how='outer', on=['object_id'])\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzu1Q_8IVwQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_train = featurize(train,train_meta)\n",
        "y = full_train['target']\n",
        "full_train =  full_train.drop(labels=['target','object_id','hostgal_specz','ra', 'decl', 'gal_l', 'gal_b','ddf'], axis=1)\n",
        "\n",
        "# Impute NAs with mean:\n",
        "train_mean = full_train.mean(axis=0)\n",
        "full_train.fillna(0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2BHjG-VxVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ============================================================\n",
        "## Model Training & OOF Predictions\n",
        "## 1st level using LGB and XGB models\n",
        "## ============================================================\n",
        "\n",
        "# Define objective functions to be used in fitting:\n",
        "def multi_weighted_logloss(y_true, y_preds):\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds\n",
        "    # Trasform y_true in dummies\n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
        "    # Exclude class 99 for now, since there is no class99 in the training set \n",
        "    # we gave a special process for that class\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    \n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return loss\n",
        "\n",
        "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    @author olivier https://www.kaggle.com/ogrellier\n",
        "    multi logloss for PLAsTiCC challenge\n",
        "    \"\"\"\n",
        "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
        "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
        "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
        "    # Trasform y_true in dummies\n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
        "    # Exclude class 99 for now, since there is no class99 in the training set\n",
        "    # we gave a special process for that class\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return 'wloss', loss, False\n",
        "\n",
        "def xgb_multi_weighted_logloss(y_predicted, y_true):\n",
        "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted)\n",
        "    return 'wloss', loss\n",
        "\n",
        "seed = 1\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 14,\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.03,\n",
        "    'subsample': .9,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'reg_alpha': .01,\n",
        "    'reg_lambda': .01,\n",
        "    'min_split_gain': 0.01,\n",
        "    'min_child_weight': 10,\n",
        "    'n_estimators': 1000,\n",
        "    'silent': -1,\n",
        "    'verbose': -1,\n",
        "    'max_depth': 3,\n",
        "    'n_jobs' : 4\n",
        "}\n",
        "\n",
        "\n",
        "xgb_params = {    \n",
        "        'objective': 'multi:softprob', \n",
        "        'eval_metric': 'mlogloss', \n",
        "        'silent': True, \n",
        "        'num_class':14,    \n",
        "        'booster': 'gbtree',\n",
        "        'n_jobs': 4,\n",
        "        'n_estimators': 1000,\n",
        "        'tree_method': 'hist',\n",
        "        'grow_policy': 'lossguide',\n",
        "        'base_score': 0.25,\n",
        "        'max_depth': 7,\n",
        "        'max_delta_step': 2,  #default=0\n",
        "        'learning_rate': 0.03,\n",
        "        'max_leaves': 11,\n",
        "        'min_child_weight': 64,\n",
        "        'gamma': 0.1, # default=\n",
        "        'subsample': 0.7,\n",
        "        'colsample_bytree': 0.68,\n",
        "        'reg_alpha': 0.01, # default=0\n",
        "        'reg_lambda': 10., # default=1\n",
        "        'seed': 538\n",
        "}\n",
        "\n",
        "\n",
        "def lgb_train_pred(train,y):\n",
        "    # Function outputs clfs, importances, and oof_preds\n",
        "    light_clfs = []\n",
        "    light_importances = pd.DataFrame()\n",
        "    lgb_oof_train = np.zeros((len(train), np.unique(y).shape[0]))   \n",
        "    w = y.value_counts()\n",
        "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
        "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
        "        trn_x, trn_y = train.iloc[trn_], y.iloc[trn_]\n",
        "        val_x, val_y = train.iloc[val_], y.iloc[val_]\n",
        "        clf = lgb.LGBMClassifier(**lgb_params)\n",
        "        clf.fit(\n",
        "            trn_x, trn_y,\n",
        "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "            eval_metric=lgb_multi_weighted_logloss,\n",
        "            verbose=100,\n",
        "            early_stopping_rounds=50,\n",
        "            sample_weight=trn_y.map(weights)\n",
        "        )\n",
        "        lgb_oof_train[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
        "        print(multi_weighted_logloss(val_y, lgb_oof_train[val_, :]))\n",
        "        imp_df = pd.DataFrame()\n",
        "        imp_df['feature'] = train.columns\n",
        "        imp_df['gain'] = clf.feature_importances_\n",
        "        imp_df['fold'] = fold_ + 1\n",
        "        light_importances = pd.concat([light_importances, imp_df], axis=0)\n",
        "        light_clfs.append(clf)   \n",
        "    return light_clfs, light_importances, lgb_oof_train\n",
        "\n",
        "def xgb_train_pred(train,y):\n",
        "    # Function outputs clfs, importances, and oof_preds\n",
        "    xg_clfs = []\n",
        "    xg_importances = pd.DataFrame()\n",
        "    xgb_oof_train = np.zeros((len(train), np.unique(y).shape[0]))   \n",
        "    w = y.value_counts()\n",
        "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
        "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
        "        trn_x, trn_y = train.iloc[trn_], y.iloc[trn_]\n",
        "        val_x, val_y = train.iloc[val_], y.iloc[val_]\n",
        "        clf = xgb.XGBClassifier(**xgb_params)\n",
        "        clf.fit(\n",
        "            trn_x, trn_y,\n",
        "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "            eval_metric=xgb_multi_weighted_logloss,\n",
        "            verbose=100,\n",
        "            early_stopping_rounds=50,\n",
        "            sample_weight=trn_y.map(weights)\n",
        "        )\n",
        "        xgb_oof_train[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
        "        print(multi_weighted_logloss(val_y, xgb_oof_train[val_, :]))\n",
        "        imp_df = pd.DataFrame()\n",
        "        imp_df['feature'] = train.columns\n",
        "        imp_df['gain'] = clf.feature_importances_\n",
        "        imp_df['fold'] = fold_ + 1\n",
        "        xg_importances = pd.concat([xg_importances, imp_df], axis=0)\n",
        "        xg_clfs.append(clf)\n",
        "    return xg_clfs, xg_importances, xgb_oof_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDH52731V2JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "light_clfs, light_importances, lgb_oof_train = lgb_train_pred(full_train,y)[0], \\\n",
        "    lgb_train_pred(full_train,y)[1], lgb_train_pred(full_train,y)[2]\n",
        "\n",
        "xg_clfs, xg_importances, xgb_oof_train = xgb_train_pred(full_train,y)[0], \\\n",
        "    xgb_train_pred(full_train,y)[1], xgb_train_pred(full_train,y)[2]    \n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_7eJMfvV4cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## =======================================================\n",
        "## Validation\n",
        "## =======================================================\n",
        "\n",
        "print('LGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=lgb_oof_train))\n",
        "print('XGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=xgb_oof_train))\n",
        "\n",
        "# define function to plot confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def model_confusion(y,model_preds):\n",
        "    unique_y = np.unique(y)\n",
        "    class_map = dict()\n",
        "    for i,val in enumerate(unique_y):\n",
        "        class_map[val] = i\n",
        "    class_names = list(sample_sub.columns[1:-1])\n",
        "    y_map = np.zeros((y.shape[0],))\n",
        "    y_map = np.array([class_map[val] for val in y])\n",
        "    cnf_matrix = confusion_matrix(y_map, np.argmax(model_preds,axis=-1))\n",
        "    np.set_printoptions(precision=2)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
        "                      title='Confusion matrix')\n",
        "    \n",
        "model_confusion(y,lgb_oof_train)\n",
        "model_confusion(y,xgb_oof_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsKMpLWkV630",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ============================================================\n",
        "## Test Set Predictions\n",
        "## 1st level using XGB and LGB model    \n",
        "## ============================================================\n",
        "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
        "    # Group by object id    \n",
        "    full_test = featurize(df_,meta_)\n",
        "    full_test = full_test.fillna(0)\n",
        "    # Make predictions\n",
        "    preds_ = None\n",
        "    for clf in clfs_:\n",
        "        if preds_ is None:\n",
        "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
        "        else:\n",
        "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
        "    # Compute preds_99 as the proba of class not being any of the others\n",
        "    # preds_99 = 0.1 gives 1.769\n",
        "    preds_99 = np.ones(preds_.shape[0])\n",
        "    for i in range(preds_.shape[1]):\n",
        "        preds_99 *= (1 - preds_[:, i])\n",
        "    # Create DataFrame from predictions\n",
        "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
        "    preds_df_['object_id'] = full_test['object_id']\n",
        "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
        "    return preds_df_ \n",
        "def test_pred(testfile,chunks,clfs):\n",
        "    start = time.time()\n",
        "    remain_df = None\n",
        "    final_preds = pd.DataFrame()\n",
        "    for i_c, df in enumerate(pd.read_csv(testfile, chunksize=chunks, iterator=True)): \n",
        "        # Check object_ids\n",
        "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
        "        unique_ids = np.unique(df['object_id'])\n",
        "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
        "        if remain_df is None:\n",
        "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
        "        else:\n",
        "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
        "        # Create remaining samples df\n",
        "        remain_df = new_remain_df\n",
        "        preds_df = predict_chunk(df_=df,\n",
        "                                 clfs_=clfs,\n",
        "                                 meta_=test_meta,\n",
        "                                 features=full_train.columns,\n",
        "                                 train_mean=train_mean)\n",
        "        final_preds = pd.concat([final_preds, preds_df], axis=0)\n",
        "        del preds_df\n",
        "        gc.collect()\n",
        "        print('%15d done in %5.1f minutes' % (chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
        "    # Compute last object in remain_df\n",
        "    preds_df = predict_chunk(df_=remain_df,\n",
        "                         clfs_=clfs,\n",
        "                         meta_=test_meta,\n",
        "                         features=full_train.columns,\n",
        "                         train_mean=train_mean)\n",
        "    final_preds = pd.concat([final_preds, preds_df], axis=0)\n",
        "    return final_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNKkXs2pV95m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=light_clfs)\n",
        "xgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=xg_clfs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hpxa5d0V_H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### RUN:\n",
        "def main():\n",
        "    full_train = featurize(train,train_meta)\n",
        "    y = full_train['target']\n",
        "    full_train =  full_train.drop(labels=['target','object_id','hostgal_specz','ra', 'decl', 'gal_l', 'gal_b','ddf'], axis=1)\n",
        "    # Impute NAs with mean:\n",
        "    train_mean = full_train.mean(axis=0)\n",
        "    full_train.fillna(0, inplace=True)\n",
        "    seed = 1\n",
        "    folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    lgb_params = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 14,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.03,\n",
        "        'subsample': .9,\n",
        "        'colsample_bytree': 0.5,\n",
        "        'reg_alpha': .01,\n",
        "        'reg_lambda': .01,\n",
        "        'min_split_gain': 0.01,\n",
        "        'min_child_weight': 10,\n",
        "        'n_estimators': 1000,\n",
        "        'silent': -1,\n",
        "        'verbose': -1,\n",
        "        'max_depth': 3,\n",
        "        'n_jobs' : 4}\n",
        "    xgb_params = {    \n",
        "            'objective': 'multi:softprob', \n",
        "            'eval_metric': 'mlogloss', \n",
        "            'silent': True, \n",
        "            'num_class':14,    \n",
        "            'booster': 'gbtree',\n",
        "            'n_jobs': 4,\n",
        "            'n_estimators': 1000,\n",
        "            'tree_method': 'hist',\n",
        "            'grow_policy': 'lossguide',\n",
        "            'base_score': 0.25,\n",
        "            'max_depth': 7,\n",
        "            'max_delta_step': 2,  #default=0\n",
        "            'learning_rate': 0.03,\n",
        "            'max_leaves': 11,\n",
        "            'min_child_weight': 64,\n",
        "            'gamma': 0.1, # default=\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.68,\n",
        "            'reg_alpha': 0.01, # default=0\n",
        "            'reg_lambda': 10., # default=1\n",
        "            'seed': seed}\n",
        "    light_clfs, light_importances, lgb_oof_train = lgb_train_pred(full_train,y)[0], \\\n",
        "        lgb_train_pred(full_train,y)[1], lgb_train_pred(full_train,y)[2]\n",
        "    xg_clfs, xg_importances, xgb_oof_train = xgb_train_pred(full_train,y)[0], \\\n",
        "        xgb_train_pred(full_train,y)[1], xgb_train_pred(full_train,y)[2]    \n",
        "    gc.collect()\n",
        "    print('LGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=lgb_oof_train))\n",
        "    print('XGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=xgb_oof_train))\n",
        "    print('starting lgb tests')\n",
        "    lgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=light_clfs)\n",
        "    print('starting xgb tests')\n",
        "    xgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=xg_clfs)\n",
        "    ## ============================================================\n",
        "    ## Stacking\n",
        "    ## 2nd level using LGB classifier\n",
        "    ## ============================================================\n",
        "    second_train = pd.concat((pd.DataFrame(xgb_oof_train), pd.DataFrame(lgb_oof_train)),axis=1)\n",
        "    second_test = pd.concat((pd.DataFrame(xgb_oof_test), pd.DataFrame(lgb_oof_test)),axis=1)\n",
        "    # Fit LGBMClassifier as second level meta model\n",
        "    second_clfs, second_importances, second_oof_train =lgb_train_pred(second_train,y)[0], \\\n",
        "        lgb_train_pred(second_train,y)[1], lgb_train_pred(second_train,y)[2]  \n",
        "    gc.collect()\n",
        "    ## ============================================================\n",
        "    ## Final submission \n",
        "    ## ============================================================\n",
        "    print('starting final tests')\n",
        "    final_preds = test_pred(testfile = 'test_set.csv', chunks = 5000000, clfs=second_clfs)\n",
        "    final_preds.to_csv('submission.csv', header=False, mode='a', index=False)\n",
        "    return [light_clfs,xg_clfs,second_clfs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aplF9eKFWCo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[light_clfs,xg_clfs,second_clfs] = main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}