{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLAsTiCC Final Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlwxOqSB8AcRnZHkrUYXr7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markerenberg/PLAsTiCC/blob/master/PLAsTiCC_Final_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7z9_s5JEe5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tsfresh\n",
        "!pip install emcee\n",
        "!pip install celerite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjvw1b2GVXj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tsfresh.feature_extraction import extract_features\n",
        "import autograd.numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import pywt\n",
        "import warnings\n",
        "import emcee\n",
        "import celerite\n",
        "from celerite import terms\n",
        "#from matplotlib.mlab import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43VroByVnLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir('C:\\\\Users\\\\marke\\\\Downloads\\\\PLAsTiCC')\n",
        "train = pd.read_csv('training_set.csv')\n",
        "train_meta = pd.read_csv('training_set_metadata.csv')\n",
        "#test_meta = pd.read_csv('test_set_metadata.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIVFacp2VsNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## =======================================================\n",
        "## Feature Engineering\n",
        "## =======================================================\n",
        "\n",
        "## subset metadata:\n",
        "#train_meta = train_meta[train_meta['target'].isin([42,52,62,67,90])]\n",
        "#train = train[train['object_id'].isin(train_meta['object_id'].unique())]\n",
        "\n",
        "gc.enable()\n",
        "\n",
        "\n",
        "#### Gaussian Process Regression ####\n",
        "\n",
        "# Estimate the MAP parameters using L-BFGS-B\n",
        "def nll(p, y, gp):\n",
        "    # Update the kernel parameters:\n",
        "    gp.set_parameter_vector(p)\n",
        "    #  Compute the loglikelihood:\n",
        "    ll = gp.log_likelihood(y,quiet=True)\n",
        "    # The scipy optimizer doesn’t play well with infinities:\n",
        "    return -ll if np.isfinite(ll) else 1e25\n",
        "def grad_nll(p, y, gp):\n",
        "    # Update the kernel parameters:\n",
        "    gp.set_parameter_vector(p)\n",
        "    #  Compute the gradient of the loglikelihood:\n",
        "    gll = gp.grad_log_likelihood(y,quiet=True)[1]\n",
        "    return -gll\n",
        "\n",
        "# set the loglikelihood:\n",
        "def lnlike(p, x, y,gp):\n",
        "    ln_a = p[0]\n",
        "    ln_b = p[1]\n",
        "    p0 = np.array([ln_a,ln_b])\n",
        "    # update kernel parameters:\n",
        "    gp.set_parameter_vector(p0)\n",
        "    # calculate the likelihood:\n",
        "    ll = gp.log_likelihood(y, quiet=True)\n",
        "    # return\n",
        "    return ll if np.isfinite(ll) else 1e25\n",
        "    \n",
        "# set the logposterior:\n",
        "def lnprob(p, x, y,gp):\n",
        "    #return lp + lnlike(p, x, y,gp) if np.isfinite(lp) else -np.inf\n",
        "    return lnlike(p, x, y,gp)\n",
        "   \n",
        "def wavelet(df):\n",
        "    # Gaussian Regression\n",
        "    result = pd.DataFrame()\n",
        "    mjds = df['mjd'].unique()\n",
        "    # Two observations per unique mjd value\n",
        "    t = np.arange(np.min(mjds),np.max(mjds),0.5)\n",
        "    if (len(t)%2) == 0:\n",
        "        t = np.insert(t,len(t),t[len(t)-1] + 0.5)\n",
        "    for obj, agg_df in df.groupby('object_id'):\n",
        "        agg_df = agg_df.sort_values(by=['mjd'])\n",
        "        X = agg_df['mjd']\n",
        "        Y = agg_df['flux']\n",
        "        Yerr = agg_df['flux_err']\n",
        "        # Start by setting hyperparamaters to unit:\n",
        "        log_sigma = 0\n",
        "        log_rho = 0\n",
        "        kernel = celerite.terms.Matern32Term(log_sigma,log_rho)\n",
        "        # According to the paper from Narayan et al, 2018, we will use the Matern 3/2 Kernel.\n",
        "        gp = celerite.GP(kernel, mean=0.0)\n",
        "        gp.compute(X,Yerr)  \n",
        "        # extract our initial guess at parameters\n",
        "        # from the celerite kernel and put it in a\n",
        "        # vector:\n",
        "        p0 = gp.get_parameter_vector()         \n",
        "        # run optimization:\n",
        "        results = minimize(nll, p0, method='L-BFGS-B', jac=grad_nll, args=(Y, gp))  \n",
        "        # set your initial guess parameters\n",
        "        # as the output from the scipy optimiser\n",
        "        # remember celerite keeps these in ln() form!\n",
        "        gp.set_parameter_vector(np.abs(results.x))\n",
        "        # Predict posterior mean and variance\n",
        "        mu, var = gp.predict(Y, t,return_var=True)\n",
        "        if(sum(np.isnan(mu)) != 0): \n",
        "            print('NANs exist in mu vector')\n",
        "            return [obj,results.x,mu]\n",
        "        # Wavelet Transform\n",
        "        # calculate wavelet transform using even numbered array\n",
        "        (cA2, cD2), (cA1, cD1) = pywt.swt(mu[1:,],'sym2',level=2)\n",
        "        obj_df = pd.DataFrame(list(cA2)+list(cA1)+list(cD2)+list(cD1)).transpose()\n",
        "        obj_df['object_id'] = obj\n",
        "        result = pd.concat([result,obj_df])\n",
        "    result.reset_index(inplace=True)\n",
        "    result.drop(\"index\",axis=1,inplace=True)\n",
        "    return result\n",
        "        \n",
        "        \n",
        "# Extract features from train set:\n",
        "def featurize(df,meta):\n",
        "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
        "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
        "    # train[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]  \n",
        "    aggs = {\n",
        "        'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
        "        'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
        "        'detected': ['mean'],\n",
        "        'flux_ratio_sq':['sum','skew'],\n",
        "        'flux_by_flux_ratio_sq':['sum','skew']}\n",
        "    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity\n",
        "    fcp = {'flux': {'longest_strike_above_mean': None,'longest_strike_below_mean': None,\n",
        "                'mean_change': None,'mean_abs_change': None,'length': None,},\n",
        "        'flux_by_flux_ratio_sq': {'longest_strike_above_mean': None,\n",
        "                                  'longest_strike_below_mean': None,},\n",
        "        'flux_passband': {'fft_coefficient': [{'coeff': 0, 'attr': 'abs'}, {'coeff': 1, 'attr': 'abs'}],\n",
        "                'kurtosis' : None, 'skewness' : None,},\n",
        "        'mjd': {'maximum': None, 'minimum': None,'mean_change': None,'mean_abs_change': None,},}\n",
        "    agg_df = df.groupby(['object_id']).agg(aggs)\n",
        "    new_columns = [\n",
        "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
        "    ]\n",
        "    agg_df.columns = new_columns\n",
        "    agg_df = agg_df.reset_index()\n",
        "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
        "    agg_df['flux_dif2'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
        "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df['flux_ratio_sq_sum']\n",
        "    agg_df['flux_dif3'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
        "    # Added DWT features (ignore warnings from pywt.wavedec):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        wavelet_df = wavelet(df)\n",
        "    agg_df = agg_df.merge(right=wavelet_df,how = 'left',on='object_id')\n",
        "    print('MERGED WAVELET DF')\n",
        "    # Run PCA\n",
        "    X = agg_df.iloc[:, 22:]\n",
        "    X_std = StandardScaler().fit_transform(X)\n",
        "    pca = PCA(n_components=200)\n",
        "    pca_results = pd.DataFrame(pca.fit_transform(X_std))\n",
        "    pca_results.columns = ['PCA' + str(x+1) for x in range(200)]\n",
        "    agg_df = agg_df.iloc[:,:22]\n",
        "    agg_df = pd.concat([agg_df,pca_results],axis=1)\n",
        "    # Add more tsfresh features with passband, flux, flux_ratio_sq:\n",
        "    agg_df_ts_flux_passband = extract_features(df, column_id='object_id', column_sort='mjd', column_kind='passband', column_value = 'flux', default_fc_parameters = fcp['flux_passband'], n_jobs=4)\n",
        "    agg_df_ts_flux = extract_features(df, column_id='object_id', column_value='flux', default_fc_parameters=fcp['flux'], n_jobs=4)\n",
        "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, column_id='object_id', column_value='flux_by_flux_ratio_sq', default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=4)\n",
        "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
        "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
        "    df_det = df[df['detected']==1].copy() \n",
        "    agg_df_mjd = extract_features(df_det, column_id='object_id', column_value = 'mjd', default_fc_parameters = fcp['mjd'], n_jobs=4)\n",
        "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd['mjd__minimum']\n",
        "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum'] \n",
        "    agg_df_ts_flux_passband = agg_df_ts_flux_passband.reset_index()\n",
        "    agg_df_ts_flux_passband.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_ts_flux = agg_df_ts_flux.reset_index()\n",
        "    agg_df_ts_flux.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_ts_flux_by_flux_ratio_sq = agg_df_ts_flux_by_flux_ratio_sq.reset_index()\n",
        "    agg_df_ts_flux_by_flux_ratio_sq.rename(columns={'id':'object_id'},inplace=True)\n",
        "    agg_df_mjd = agg_df_mjd.reset_index()\n",
        "    agg_df_mjd.rename(columns={'id':'object_id'},inplace=True) \n",
        "    agg_df_ts = pd.concat([agg_df, \n",
        "                           agg_df_ts_flux_passband.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_ts_flux.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_ts_flux_by_flux_ratio_sq.drop(labels=['object_id'], axis=1), \n",
        "                           agg_df_mjd.drop(labels=['object_id'], axis=1)], axis=1).reset_index()\n",
        "    if 'index' in agg_df_ts:\n",
        "       del agg_df_ts['index'] \n",
        "    result = agg_df_ts.merge(right=meta, how='outer', on=['object_id'])\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzu1Q_8IVwQ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "93b0bf6d-320a-4f91-8df6-5e98598bc47b"
      },
      "source": [
        "full_train = featurize(train,train_meta)\n",
        "y = full_train['target']\n",
        "full_train =  full_train.drop(labels=['target','object_id','hostgal_specz','ra', 'decl', 'gal_l', 'gal_b','ddf'], axis=1)\n",
        "\n",
        "# Impute NAs with mean:\n",
        "train_mean = full_train.mean(axis=0)\n",
        "full_train.fillna(0, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MERGED WAVELET DF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 20/20 [00:17<00:00,  1.87it/s]\n",
            "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  4.22it/s]\n",
            "Feature Extraction: 100%|██████████| 20/20 [00:02<00:00,  8.59it/s]\n",
            "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 12.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2BHjG-VxVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ============================================================\n",
        "## Model Training & OOF Predictions\n",
        "## 1st level using LGB and XGB models\n",
        "## ============================================================\n",
        "\n",
        "# Define objective functions to be used in fitting:\n",
        "def multi_weighted_logloss(y_true, y_preds):\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds\n",
        "    # Trasform y_true in dummies\n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
        "    # Exclude class 99 for now, since there is no class99 in the training set \n",
        "    # we gave a special process for that class\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    \n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return loss\n",
        "\n",
        "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
        "    \"\"\"\n",
        "    @author olivier https://www.kaggle.com/ogrellier\n",
        "    multi logloss for PLAsTiCC challenge\n",
        "    \"\"\"\n",
        "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
        "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
        "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
        "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
        "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
        "    if len(np.unique(y_true)) > 14:\n",
        "        classes.append(99)\n",
        "        class_weight[99] = 2\n",
        "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
        "    # Trasform y_true in dummies\n",
        "    y_ohe = pd.get_dummies(y_true)\n",
        "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
        "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
        "    # Transform to log\n",
        "    y_p_log = np.log(y_p)\n",
        "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
        "    # Exclude class 99 for now, since there is no class99 in the training set\n",
        "    # we gave a special process for that class\n",
        "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
        "    # Get the number of positives for each class\n",
        "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
        "    # Weight average and divide by the number of positives\n",
        "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
        "    y_w = y_log_ones * class_arr / nb_pos\n",
        "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
        "    return 'wloss', loss, False\n",
        "\n",
        "def xgb_multi_weighted_logloss(y_predicted, y_true):\n",
        "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted)\n",
        "    return 'wloss', loss\n",
        "\n",
        "seed = 1\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 14,\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.03,\n",
        "    'subsample': .9,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'reg_alpha': .01,\n",
        "    'reg_lambda': .01,\n",
        "    'min_split_gain': 0.01,\n",
        "    'min_child_weight': 10,\n",
        "    'n_estimators': 1000,\n",
        "    'silent': -1,\n",
        "    'verbose': -1,\n",
        "    'max_depth': 3,\n",
        "    'n_jobs' : 4\n",
        "}\n",
        "\n",
        "\n",
        "xgb_params = {    \n",
        "        'objective': 'multi:softprob', \n",
        "        'eval_metric': 'mlogloss', \n",
        "        'num_class':14,    \n",
        "        'booster': 'gbtree',\n",
        "        'n_jobs': 4,\n",
        "        'n_estimators': 1000,\n",
        "        'tree_method': 'hist',\n",
        "        'grow_policy': 'lossguide',\n",
        "        'base_score': 0.25,\n",
        "        'max_depth': 7,\n",
        "        'max_delta_step': 2,  #default=0\n",
        "        'learning_rate': 0.03,\n",
        "        'max_leaves': 11,\n",
        "        'min_child_weight': 64,\n",
        "        'gamma': 0.1, # default=\n",
        "        'subsample': 0.7,\n",
        "        'colsample_bytree': 0.68,\n",
        "        'reg_alpha': 0.01, # default=0\n",
        "        'reg_lambda': 10., # default=1\n",
        "        'seed': 1234,\n",
        "        'verbosity':0\n",
        "}\n",
        "\n",
        "\n",
        "def lgb_train_pred(train,y):\n",
        "    # Function outputs clfs, importances, and oof_preds\n",
        "    light_clfs = []\n",
        "    light_importances = pd.DataFrame()\n",
        "    lgb_oof_train = np.zeros((len(train), np.unique(y).shape[0]))   \n",
        "    w = y.value_counts()\n",
        "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
        "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
        "        trn_x, trn_y = train.iloc[trn_], y.iloc[trn_]\n",
        "        val_x, val_y = train.iloc[val_], y.iloc[val_]\n",
        "        clf = lgb.LGBMClassifier(**lgb_params)\n",
        "        clf.fit(\n",
        "            trn_x, trn_y,\n",
        "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "            eval_metric=lgb_multi_weighted_logloss,\n",
        "            verbose=100,\n",
        "            early_stopping_rounds=50,\n",
        "            sample_weight=trn_y.map(weights)\n",
        "        )\n",
        "        lgb_oof_train[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
        "        print(multi_weighted_logloss(val_y, lgb_oof_train[val_, :]))\n",
        "        imp_df = pd.DataFrame()\n",
        "        imp_df['feature'] = train.columns\n",
        "        imp_df['gain'] = clf.feature_importances_\n",
        "        imp_df['fold'] = fold_ + 1\n",
        "        light_importances = pd.concat([light_importances, imp_df], axis=0)\n",
        "        light_clfs.append(clf)   \n",
        "    return light_clfs, light_importances, lgb_oof_train\n",
        "\n",
        "def xgb_train_pred(train,y):\n",
        "    # Function outputs clfs, importances, and oof_preds\n",
        "    xg_clfs = []\n",
        "    xg_importances = pd.DataFrame()\n",
        "    xgb_oof_train = np.zeros((len(train), np.unique(y).shape[0]))   \n",
        "    w = y.value_counts()\n",
        "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
        "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
        "        trn_x, trn_y = train.iloc[trn_], y.iloc[trn_]\n",
        "        val_x, val_y = train.iloc[val_], y.iloc[val_]\n",
        "        clf = xgb.XGBClassifier(**xgb_params)\n",
        "        clf.fit(\n",
        "            trn_x, trn_y,\n",
        "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
        "            eval_metric=xgb_multi_weighted_logloss,\n",
        "            verbose=100,\n",
        "            early_stopping_rounds=50,\n",
        "            sample_weight=trn_y.map(weights)\n",
        "        )\n",
        "        xgb_oof_train[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
        "        print(multi_weighted_logloss(val_y, xgb_oof_train[val_, :]))\n",
        "        imp_df = pd.DataFrame()\n",
        "        imp_df['feature'] = train.columns\n",
        "        imp_df['gain'] = clf.feature_importances_\n",
        "        imp_df['fold'] = fold_ + 1\n",
        "        xg_importances = pd.concat([xg_importances, imp_df], axis=0)\n",
        "        xg_clfs.append(clf)\n",
        "    return xg_clfs, xg_importances, xgb_oof_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDH52731V2JN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31c485fe-f871-4f02-cf73-7b9aa5e4643b"
      },
      "source": [
        "light_clfs, light_importances, lgb_oof_train = lgb_train_pred(full_train,y)[0], \\\n",
        "    lgb_train_pred(full_train,y)[1], lgb_train_pred(full_train,y)[2]\n",
        "\n",
        "xg_clfs, xg_importances, xgb_oof_train = xgb_train_pred(full_train,y)[0], \\\n",
        "    xgb_train_pred(full_train,y)[1], xgb_train_pred(full_train,y)[2]    \n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.775697\ttraining's wloss: 0.766237\tvalid_1's multi_logloss: 1.15303\tvalid_1's wloss: 0.984548\n",
            "[200]\ttraining's multi_logloss: 0.513202\ttraining's wloss: 0.500164\tvalid_1's multi_logloss: 0.932452\tvalid_1's wloss: 0.808699\n",
            "[300]\ttraining's multi_logloss: 0.391878\ttraining's wloss: 0.378465\tvalid_1's multi_logloss: 0.840636\tvalid_1's wloss: 0.760391\n",
            "[400]\ttraining's multi_logloss: 0.318469\ttraining's wloss: 0.305118\tvalid_1's multi_logloss: 0.791891\tvalid_1's wloss: 0.753999\n",
            "Early stopping, best iteration is:\n",
            "[431]\ttraining's multi_logloss: 0.299611\ttraining's wloss: 0.286515\tvalid_1's multi_logloss: 0.778995\tvalid_1's wloss: 0.752569\n",
            "0.7525686440148562\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.791012\ttraining's wloss: 0.781955\tvalid_1's multi_logloss: 1.14922\tvalid_1's wloss: 0.943925\n",
            "[200]\ttraining's multi_logloss: 0.525777\ttraining's wloss: 0.512827\tvalid_1's multi_logloss: 0.920606\tvalid_1's wloss: 0.759917\n",
            "[300]\ttraining's multi_logloss: 0.400631\ttraining's wloss: 0.386712\tvalid_1's multi_logloss: 0.827488\tvalid_1's wloss: 0.714633\n",
            "[400]\ttraining's multi_logloss: 0.32343\ttraining's wloss: 0.309676\tvalid_1's multi_logloss: 0.773588\tvalid_1's wloss: 0.696115\n",
            "[500]\ttraining's multi_logloss: 0.269034\ttraining's wloss: 0.25603\tvalid_1's multi_logloss: 0.736128\tvalid_1's wloss: 0.692152\n",
            "Early stopping, best iteration is:\n",
            "[478]\ttraining's multi_logloss: 0.27985\ttraining's wloss: 0.26669\tvalid_1's multi_logloss: 0.743351\tvalid_1's wloss: 0.690841\n",
            "0.6908408801659767\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.790712\ttraining's wloss: 0.782351\tvalid_1's multi_logloss: 1.13468\tvalid_1's wloss: 0.90895\n",
            "[200]\ttraining's multi_logloss: 0.52165\ttraining's wloss: 0.50943\tvalid_1's multi_logloss: 0.901404\tvalid_1's wloss: 0.710518\n",
            "[300]\ttraining's multi_logloss: 0.39563\ttraining's wloss: 0.382502\tvalid_1's multi_logloss: 0.809686\tvalid_1's wloss: 0.660538\n",
            "[400]\ttraining's multi_logloss: 0.318857\ttraining's wloss: 0.306094\tvalid_1's multi_logloss: 0.760445\tvalid_1's wloss: 0.655098\n",
            "Early stopping, best iteration is:\n",
            "[383]\ttraining's multi_logloss: 0.329472\ttraining's wloss: 0.316569\tvalid_1's multi_logloss: 0.767124\tvalid_1's wloss: 0.65421\n",
            "0.6542103918279535\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.781032\ttraining's wloss: 0.768465\tvalid_1's multi_logloss: 1.15866\tvalid_1's wloss: 0.982835\n",
            "[200]\ttraining's multi_logloss: 0.513171\ttraining's wloss: 0.49737\tvalid_1's multi_logloss: 0.937093\tvalid_1's wloss: 0.793838\n",
            "[300]\ttraining's multi_logloss: 0.388639\ttraining's wloss: 0.37273\tvalid_1's multi_logloss: 0.846997\tvalid_1's wloss: 0.743465\n",
            "[400]\ttraining's multi_logloss: 0.313203\ttraining's wloss: 0.298046\tvalid_1's multi_logloss: 0.796765\tvalid_1's wloss: 0.73269\n",
            "Early stopping, best iteration is:\n",
            "[419]\ttraining's multi_logloss: 0.301484\ttraining's wloss: 0.286521\tvalid_1's multi_logloss: 0.789382\tvalid_1's wloss: 0.731264\n",
            "0.7312636961380372\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.7864\ttraining's wloss: 0.777711\tvalid_1's multi_logloss: 1.15595\tvalid_1's wloss: 0.929197\n",
            "[200]\ttraining's multi_logloss: 0.516754\ttraining's wloss: 0.504736\tvalid_1's multi_logloss: 0.935253\tvalid_1's wloss: 0.747083\n",
            "[300]\ttraining's multi_logloss: 0.394772\ttraining's wloss: 0.381648\tvalid_1's multi_logloss: 0.849644\tvalid_1's wloss: 0.708902\n",
            "[400]\ttraining's multi_logloss: 0.319198\ttraining's wloss: 0.306319\tvalid_1's multi_logloss: 0.799238\tvalid_1's wloss: 0.696574\n",
            "[500]\ttraining's multi_logloss: 0.266011\ttraining's wloss: 0.253802\tvalid_1's multi_logloss: 0.763414\tvalid_1's wloss: 0.694556\n",
            "Early stopping, best iteration is:\n",
            "[454]\ttraining's multi_logloss: 0.288429\ttraining's wloss: 0.275907\tvalid_1's multi_logloss: 0.777945\tvalid_1's wloss: 0.693988\n",
            "0.6939881372211604\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.775697\ttraining's wloss: 0.766237\tvalid_1's multi_logloss: 1.15303\tvalid_1's wloss: 0.984548\n",
            "[200]\ttraining's multi_logloss: 0.513202\ttraining's wloss: 0.500164\tvalid_1's multi_logloss: 0.932452\tvalid_1's wloss: 0.808699\n",
            "[300]\ttraining's multi_logloss: 0.391878\ttraining's wloss: 0.378465\tvalid_1's multi_logloss: 0.840636\tvalid_1's wloss: 0.760391\n",
            "[400]\ttraining's multi_logloss: 0.318469\ttraining's wloss: 0.305118\tvalid_1's multi_logloss: 0.791891\tvalid_1's wloss: 0.753999\n",
            "Early stopping, best iteration is:\n",
            "[431]\ttraining's multi_logloss: 0.299611\ttraining's wloss: 0.286515\tvalid_1's multi_logloss: 0.778995\tvalid_1's wloss: 0.752569\n",
            "0.7525686440148562\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.791012\ttraining's wloss: 0.781955\tvalid_1's multi_logloss: 1.14922\tvalid_1's wloss: 0.943925\n",
            "[200]\ttraining's multi_logloss: 0.525777\ttraining's wloss: 0.512827\tvalid_1's multi_logloss: 0.920606\tvalid_1's wloss: 0.759917\n",
            "[300]\ttraining's multi_logloss: 0.400631\ttraining's wloss: 0.386712\tvalid_1's multi_logloss: 0.827488\tvalid_1's wloss: 0.714633\n",
            "[400]\ttraining's multi_logloss: 0.32343\ttraining's wloss: 0.309676\tvalid_1's multi_logloss: 0.773588\tvalid_1's wloss: 0.696115\n",
            "[500]\ttraining's multi_logloss: 0.269034\ttraining's wloss: 0.25603\tvalid_1's multi_logloss: 0.736128\tvalid_1's wloss: 0.692152\n",
            "Early stopping, best iteration is:\n",
            "[478]\ttraining's multi_logloss: 0.27985\ttraining's wloss: 0.26669\tvalid_1's multi_logloss: 0.743351\tvalid_1's wloss: 0.690841\n",
            "0.6908408801659767\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.790712\ttraining's wloss: 0.782351\tvalid_1's multi_logloss: 1.13468\tvalid_1's wloss: 0.90895\n",
            "[200]\ttraining's multi_logloss: 0.52165\ttraining's wloss: 0.50943\tvalid_1's multi_logloss: 0.901404\tvalid_1's wloss: 0.710518\n",
            "[300]\ttraining's multi_logloss: 0.39563\ttraining's wloss: 0.382502\tvalid_1's multi_logloss: 0.809686\tvalid_1's wloss: 0.660538\n",
            "[400]\ttraining's multi_logloss: 0.318857\ttraining's wloss: 0.306094\tvalid_1's multi_logloss: 0.760445\tvalid_1's wloss: 0.655098\n",
            "Early stopping, best iteration is:\n",
            "[383]\ttraining's multi_logloss: 0.329472\ttraining's wloss: 0.316569\tvalid_1's multi_logloss: 0.767124\tvalid_1's wloss: 0.65421\n",
            "0.6542103918279535\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.781032\ttraining's wloss: 0.768465\tvalid_1's multi_logloss: 1.15866\tvalid_1's wloss: 0.982835\n",
            "[200]\ttraining's multi_logloss: 0.513171\ttraining's wloss: 0.49737\tvalid_1's multi_logloss: 0.937093\tvalid_1's wloss: 0.793838\n",
            "[300]\ttraining's multi_logloss: 0.388639\ttraining's wloss: 0.37273\tvalid_1's multi_logloss: 0.846997\tvalid_1's wloss: 0.743465\n",
            "[400]\ttraining's multi_logloss: 0.313203\ttraining's wloss: 0.298046\tvalid_1's multi_logloss: 0.796765\tvalid_1's wloss: 0.73269\n",
            "Early stopping, best iteration is:\n",
            "[419]\ttraining's multi_logloss: 0.301484\ttraining's wloss: 0.286521\tvalid_1's multi_logloss: 0.789382\tvalid_1's wloss: 0.731264\n",
            "0.7312636961380372\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.7864\ttraining's wloss: 0.777711\tvalid_1's multi_logloss: 1.15595\tvalid_1's wloss: 0.929197\n",
            "[200]\ttraining's multi_logloss: 0.516754\ttraining's wloss: 0.504736\tvalid_1's multi_logloss: 0.935253\tvalid_1's wloss: 0.747083\n",
            "[300]\ttraining's multi_logloss: 0.394772\ttraining's wloss: 0.381648\tvalid_1's multi_logloss: 0.849644\tvalid_1's wloss: 0.708902\n",
            "[400]\ttraining's multi_logloss: 0.319198\ttraining's wloss: 0.306319\tvalid_1's multi_logloss: 0.799238\tvalid_1's wloss: 0.696574\n",
            "[500]\ttraining's multi_logloss: 0.266011\ttraining's wloss: 0.253802\tvalid_1's multi_logloss: 0.763414\tvalid_1's wloss: 0.694556\n",
            "Early stopping, best iteration is:\n",
            "[454]\ttraining's multi_logloss: 0.288429\ttraining's wloss: 0.275907\tvalid_1's multi_logloss: 0.777945\tvalid_1's wloss: 0.693988\n",
            "0.6939881372211604\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.775697\ttraining's wloss: 0.766237\tvalid_1's multi_logloss: 1.15303\tvalid_1's wloss: 0.984548\n",
            "[200]\ttraining's multi_logloss: 0.513202\ttraining's wloss: 0.500164\tvalid_1's multi_logloss: 0.932452\tvalid_1's wloss: 0.808699\n",
            "[300]\ttraining's multi_logloss: 0.391878\ttraining's wloss: 0.378465\tvalid_1's multi_logloss: 0.840636\tvalid_1's wloss: 0.760391\n",
            "[400]\ttraining's multi_logloss: 0.318469\ttraining's wloss: 0.305118\tvalid_1's multi_logloss: 0.791891\tvalid_1's wloss: 0.753999\n",
            "Early stopping, best iteration is:\n",
            "[431]\ttraining's multi_logloss: 0.299611\ttraining's wloss: 0.286515\tvalid_1's multi_logloss: 0.778995\tvalid_1's wloss: 0.752569\n",
            "0.7525686440148562\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.791012\ttraining's wloss: 0.781955\tvalid_1's multi_logloss: 1.14922\tvalid_1's wloss: 0.943925\n",
            "[200]\ttraining's multi_logloss: 0.525777\ttraining's wloss: 0.512827\tvalid_1's multi_logloss: 0.920606\tvalid_1's wloss: 0.759917\n",
            "[300]\ttraining's multi_logloss: 0.400631\ttraining's wloss: 0.386712\tvalid_1's multi_logloss: 0.827488\tvalid_1's wloss: 0.714633\n",
            "[400]\ttraining's multi_logloss: 0.32343\ttraining's wloss: 0.309676\tvalid_1's multi_logloss: 0.773588\tvalid_1's wloss: 0.696115\n",
            "[500]\ttraining's multi_logloss: 0.269034\ttraining's wloss: 0.25603\tvalid_1's multi_logloss: 0.736128\tvalid_1's wloss: 0.692152\n",
            "Early stopping, best iteration is:\n",
            "[478]\ttraining's multi_logloss: 0.27985\ttraining's wloss: 0.26669\tvalid_1's multi_logloss: 0.743351\tvalid_1's wloss: 0.690841\n",
            "0.6908408801659767\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.790712\ttraining's wloss: 0.782351\tvalid_1's multi_logloss: 1.13468\tvalid_1's wloss: 0.90895\n",
            "[200]\ttraining's multi_logloss: 0.52165\ttraining's wloss: 0.50943\tvalid_1's multi_logloss: 0.901404\tvalid_1's wloss: 0.710518\n",
            "[300]\ttraining's multi_logloss: 0.39563\ttraining's wloss: 0.382502\tvalid_1's multi_logloss: 0.809686\tvalid_1's wloss: 0.660538\n",
            "[400]\ttraining's multi_logloss: 0.318857\ttraining's wloss: 0.306094\tvalid_1's multi_logloss: 0.760445\tvalid_1's wloss: 0.655098\n",
            "Early stopping, best iteration is:\n",
            "[383]\ttraining's multi_logloss: 0.329472\ttraining's wloss: 0.316569\tvalid_1's multi_logloss: 0.767124\tvalid_1's wloss: 0.65421\n",
            "0.6542103918279535\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.781032\ttraining's wloss: 0.768465\tvalid_1's multi_logloss: 1.15866\tvalid_1's wloss: 0.982835\n",
            "[200]\ttraining's multi_logloss: 0.513171\ttraining's wloss: 0.49737\tvalid_1's multi_logloss: 0.937093\tvalid_1's wloss: 0.793838\n",
            "[300]\ttraining's multi_logloss: 0.388639\ttraining's wloss: 0.37273\tvalid_1's multi_logloss: 0.846997\tvalid_1's wloss: 0.743465\n",
            "[400]\ttraining's multi_logloss: 0.313203\ttraining's wloss: 0.298046\tvalid_1's multi_logloss: 0.796765\tvalid_1's wloss: 0.73269\n",
            "Early stopping, best iteration is:\n",
            "[419]\ttraining's multi_logloss: 0.301484\ttraining's wloss: 0.286521\tvalid_1's multi_logloss: 0.789382\tvalid_1's wloss: 0.731264\n",
            "0.7312636961380372\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.7864\ttraining's wloss: 0.777711\tvalid_1's multi_logloss: 1.15595\tvalid_1's wloss: 0.929197\n",
            "[200]\ttraining's multi_logloss: 0.516754\ttraining's wloss: 0.504736\tvalid_1's multi_logloss: 0.935253\tvalid_1's wloss: 0.747083\n",
            "[300]\ttraining's multi_logloss: 0.394772\ttraining's wloss: 0.381648\tvalid_1's multi_logloss: 0.849644\tvalid_1's wloss: 0.708902\n",
            "[400]\ttraining's multi_logloss: 0.319198\ttraining's wloss: 0.306319\tvalid_1's multi_logloss: 0.799238\tvalid_1's wloss: 0.696574\n",
            "[500]\ttraining's multi_logloss: 0.266011\ttraining's wloss: 0.253802\tvalid_1's multi_logloss: 0.763414\tvalid_1's wloss: 0.694556\n",
            "Early stopping, best iteration is:\n",
            "[454]\ttraining's multi_logloss: 0.288429\ttraining's wloss: 0.275907\tvalid_1's multi_logloss: 0.777945\tvalid_1's wloss: 0.693988\n",
            "0.6939881372211604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
            "  if getattr(data, 'base', None) is not None and \\\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-mlogloss:2.59248\tvalidation_1-mlogloss:2.59575\tvalidation_0-wloss:2.58668\tvalidation_1-wloss:2.59403\n",
            "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
            "\n",
            "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.826045\tvalidation_1-mlogloss:1.00886\tvalidation_0-wloss:0.575482\tvalidation_1-wloss:0.93974\n",
            "[200]\tvalidation_0-mlogloss:0.5361\tvalidation_1-mlogloss:0.799439\tvalidation_0-wloss:0.311686\tvalidation_1-wloss:0.783235\n",
            "[300]\tvalidation_0-mlogloss:0.400366\tvalidation_1-mlogloss:0.725138\tvalidation_0-wloss:0.212188\tvalidation_1-wloss:0.764141\n",
            "Stopping. Best iteration:\n",
            "[326]\tvalidation_0-mlogloss:0.374719\tvalidation_1-mlogloss:0.712953\tvalidation_0-wloss:0.195132\tvalidation_1-wloss:0.763659\n",
            "\n",
            "0.7636592026659632\n",
            "[0]\tvalidation_0-mlogloss:2.59298\tvalidation_1-mlogloss:2.59601\tvalidation_0-wloss:2.58875\tvalidation_1-wloss:2.59515\n",
            "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
            "\n",
            "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.829067\tvalidation_1-mlogloss:0.992232\tvalidation_0-wloss:0.582132\tvalidation_1-wloss:0.89368\n",
            "[200]\tvalidation_0-mlogloss:0.535812\tvalidation_1-mlogloss:0.775636\tvalidation_0-wloss:0.314665\tvalidation_1-wloss:0.724158\n",
            "[300]\tvalidation_0-mlogloss:0.400381\tvalidation_1-mlogloss:0.700807\tvalidation_0-wloss:0.213687\tvalidation_1-wloss:0.697723\n",
            "Stopping. Best iteration:\n",
            "[305]\tvalidation_0-mlogloss:0.395456\tvalidation_1-mlogloss:0.698113\tvalidation_0-wloss:0.210223\tvalidation_1-wloss:0.696386\n",
            "\n",
            "0.6963855201977721\n",
            "[0]\tvalidation_0-mlogloss:2.59234\tvalidation_1-mlogloss:2.59474\tvalidation_0-wloss:2.5876\tvalidation_1-wloss:2.59167\n",
            "Multiple eval metrics have been passed: 'validation_1-wloss' will be used for early stopping.\n",
            "\n",
            "Will train until validation_1-wloss hasn't improved in 50 rounds.\n",
            "[100]\tvalidation_0-mlogloss:0.82953\tvalidation_1-mlogloss:0.979211\tvalidation_0-wloss:0.582296\tvalidation_1-wloss:0.851801\n",
            "[200]\tvalidation_0-mlogloss:0.536576\tvalidation_1-mlogloss:0.766245\tvalidation_0-wloss:0.314466\tvalidation_1-wloss:0.685767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_7eJMfvV4cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## =======================================================\n",
        "## Validation\n",
        "## =======================================================\n",
        "\n",
        "print('LGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=lgb_oof_train))\n",
        "print('XGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=xgb_oof_train))\n",
        "\n",
        "# define function to plot confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def model_confusion(y,model_preds):\n",
        "    unique_y = np.unique(y)\n",
        "    class_map = dict()\n",
        "    for i,val in enumerate(unique_y):\n",
        "        class_map[val] = i\n",
        "    class_names = list(sample_sub.columns[1:-1])\n",
        "    y_map = np.zeros((y.shape[0],))\n",
        "    y_map = np.array([class_map[val] for val in y])\n",
        "    cnf_matrix = confusion_matrix(y_map, np.argmax(model_preds,axis=-1))\n",
        "    np.set_printoptions(precision=2)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
        "                      title='Confusion matrix')\n",
        "    \n",
        "model_confusion(y,lgb_oof_train)\n",
        "model_confusion(y,xgb_oof_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsKMpLWkV630",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ============================================================\n",
        "## Test Set Predictions\n",
        "## 1st level using XGB and LGB model    \n",
        "## ============================================================\n",
        "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
        "    # Group by object id    \n",
        "    full_test = featurize(df_,meta_)\n",
        "    full_test = full_test.fillna(0)\n",
        "    # Make predictions\n",
        "    preds_ = None\n",
        "    for clf in clfs_:\n",
        "        if preds_ is None:\n",
        "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
        "        else:\n",
        "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
        "    # Compute preds_99 as the proba of class not being any of the others\n",
        "    # preds_99 = 0.1 gives 1.769\n",
        "    preds_99 = np.ones(preds_.shape[0])\n",
        "    for i in range(preds_.shape[1]):\n",
        "        preds_99 *= (1 - preds_[:, i])\n",
        "    # Create DataFrame from predictions\n",
        "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
        "    preds_df_['object_id'] = full_test['object_id']\n",
        "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
        "    return preds_df_ \n",
        "def test_pred(testfile,chunks,clfs):\n",
        "    start = time.time()\n",
        "    remain_df = None\n",
        "    final_preds = pd.DataFrame()\n",
        "    for i_c, df in enumerate(pd.read_csv(testfile, chunksize=chunks, iterator=True)): \n",
        "        # Check object_ids\n",
        "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
        "        unique_ids = np.unique(df['object_id'])\n",
        "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
        "        if remain_df is None:\n",
        "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
        "        else:\n",
        "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
        "        # Create remaining samples df\n",
        "        remain_df = new_remain_df\n",
        "        preds_df = predict_chunk(df_=df,\n",
        "                                 clfs_=clfs,\n",
        "                                 meta_=test_meta,\n",
        "                                 features=full_train.columns,\n",
        "                                 train_mean=train_mean)\n",
        "        final_preds = pd.concat([final_preds, preds_df], axis=0)\n",
        "        del preds_df\n",
        "        gc.collect()\n",
        "        print('%15d done in %5.1f minutes' % (chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
        "    # Compute last object in remain_df\n",
        "    preds_df = predict_chunk(df_=remain_df,\n",
        "                         clfs_=clfs,\n",
        "                         meta_=test_meta,\n",
        "                         features=full_train.columns,\n",
        "                         train_mean=train_mean)\n",
        "    final_preds = pd.concat([final_preds, preds_df], axis=0)\n",
        "    return final_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNKkXs2pV95m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create test set predictions using 1st level LGB/XGB models\n",
        "lgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=light_clfs)\n",
        "xgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=xg_clfs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hpxa5d0V_H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### RUN:\n",
        "def main():\n",
        "    full_train = featurize(train,train_meta)\n",
        "    y = full_train['target']\n",
        "    full_train =  full_train.drop(labels=['target','object_id','hostgal_specz','ra', 'decl', 'gal_l', 'gal_b','ddf'], axis=1)\n",
        "    # Impute NAs with mean:\n",
        "    train_mean = full_train.mean(axis=0)\n",
        "    full_train.fillna(0, inplace=True)\n",
        "    seed = 1\n",
        "    folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    lgb_params = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 14,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.03,\n",
        "        'subsample': .9,\n",
        "        'colsample_bytree': 0.5,\n",
        "        'reg_alpha': .01,\n",
        "        'reg_lambda': .01,\n",
        "        'min_split_gain': 0.01,\n",
        "        'min_child_weight': 10,\n",
        "        'n_estimators': 1000,\n",
        "        'silent': -1,\n",
        "        'verbose': -1,\n",
        "        'max_depth': 3,\n",
        "        'n_jobs' : 4}\n",
        "    xgb_params = {    \n",
        "            'objective': 'multi:softprob', \n",
        "            'eval_metric': 'mlogloss', \n",
        "            'silent': True, \n",
        "            'num_class':14,    \n",
        "            'booster': 'gbtree',\n",
        "            'n_jobs': 4,\n",
        "            'n_estimators': 1000,\n",
        "            'tree_method': 'hist',\n",
        "            'grow_policy': 'lossguide',\n",
        "            'base_score': 0.25,\n",
        "            'max_depth': 7,\n",
        "            'max_delta_step': 2,  #default=0\n",
        "            'learning_rate': 0.03,\n",
        "            'max_leaves': 11,\n",
        "            'min_child_weight': 64,\n",
        "            'gamma': 0.1, # default=\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.68,\n",
        "            'reg_alpha': 0.01, # default=0\n",
        "            'reg_lambda': 10., # default=1\n",
        "            'seed': seed}\n",
        "    light_clfs, light_importances, lgb_oof_train = lgb_train_pred(full_train,y)[0], \\\n",
        "        lgb_train_pred(full_train,y)[1], lgb_train_pred(full_train,y)[2]\n",
        "    xg_clfs, xg_importances, xgb_oof_train = xgb_train_pred(full_train,y)[0], \\\n",
        "        xgb_train_pred(full_train,y)[1], xgb_train_pred(full_train,y)[2]    \n",
        "    gc.collect()\n",
        "    print('LGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=lgb_oof_train))\n",
        "    print('XGB MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=xgb_oof_train))\n",
        "    print('starting lgb tests')\n",
        "    lgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=light_clfs)\n",
        "    print('starting xgb tests')\n",
        "    xgb_oof_test = test_pred(testfile = 'test_set.csv',chunks = 5000000, clfs=xg_clfs)\n",
        "    ## ============================================================\n",
        "    ## Stacking\n",
        "    ## 2nd level using LGB classifier\n",
        "    ## ============================================================\n",
        "    second_train = pd.concat((pd.DataFrame(xgb_oof_train), pd.DataFrame(lgb_oof_train)),axis=1)\n",
        "    second_test = pd.concat((pd.DataFrame(xgb_oof_test), pd.DataFrame(lgb_oof_test)),axis=1)\n",
        "    # Fit LGBMClassifier as second level meta model\n",
        "    second_clfs, second_importances, second_oof_train =lgb_train_pred(second_train,y)[0], \\\n",
        "        lgb_train_pred(second_train,y)[1], lgb_train_pred(second_train,y)[2]  \n",
        "    gc.collect()\n",
        "    ## ============================================================\n",
        "    ## Evaluation\n",
        "    ## Print classification report for final model\n",
        "    ## ============================================================\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y, second_oof_train))\n",
        "\n",
        "    ## ============================================================\n",
        "    ## Final submission \n",
        "    ## ============================================================\n",
        "    print('starting final tests')\n",
        "    final_preds = test_pred(testfile = 'test_set.csv', chunks = 5000000, clfs=second_clfs)\n",
        "    final_preds.to_csv('submission.csv', header=False, mode='a', index=False)\n",
        "    return [light_clfs,xg_clfs,second_clfs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aplF9eKFWCo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[light_clfs,xg_clfs,second_clfs] = main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}